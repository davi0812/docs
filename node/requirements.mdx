---
title: 'Infera Lite'
description: 
---

Infera Lite is an easy-to-use chrome extension UI that connects users to a decentralized AI inference network, allowing them to contribute their GPU for AI model inference. This guide covers the extension’s features and navigation.

---

## Download Infera lite

1. Go to the [Chrome Web Store](https://chromewebstore.google.com/detail/infera-lite/ffoccnmddajjohmmkccnkobelobgcdmp)
2. Click **"Add to your Chrome"** to install onto your browser.

---
## Onboarding pages will pop up once you add extension
---
## Get started 

![Welcome](images/Welcome.png)
---

## Choose your operating system

![Choose-OS](images/Choose-OS.png)

---

## Install Ollama

![Ollama](images/Ollama.png)

---

## Follow the infrastruction of your Operating system

![Apple](images/Apple.png)

---

## You're all set!

![Set](images/Set.png)

---


## Infera Lite UI

Upon opening the extension, users will see a dashboard that includes key components related to their node’s performance, points earned, and system status. Below is a breakdown of the UI elements:


### Home screen
---
The **Home screen** is the landing page of the extension and gives an overview of the node’s current status.

![Main-page](images/main-page.png)

- **Start/Stop Button**: Allows you to toggle the node on or off.
- **Active Status**: Indicates whether the node is running, displaying either “Active” (active and awaiting jobs) or “Inactive” (node deactivated).
- **Points**: Displays the points earned from processed tasks and uptime on the network.
- **Uptime**: Shows the node’s total uptime.


### Reputation & Node details
---
This section provides a real-time snapshot of your node’s performance:

![stats](images/stats.png)

- **Device details**: Displays information about the node’s GPU, CPU, RAM, VRAM, and available RAM.
- **Tasks Completed**: Shows the number of inference tasks your node has processed.
- **Reputation**: A pie chart representing your node’s reputation on the Infera network based on speed and reliability.


### Models page
---
This page lists all the LLM models that users can install to support the Infera network, starting with every state-of-the-art open-source model:

![models](images/models.png)

- **Llama 3.1 8B** 
- **Llama 3.2 1B / 3B**
- **Phi 3.5 4B**
- **Mistral 7B**
- **Qwen 2 8B**
- **Gemma 2 9B**
- **Yi large 9B**


### Privacy Assurance
---
- **No Personal Data:** We don't collect or store your personal information.
- **Encryption:** All communications are encrypted for security.
- **Local Processing:** All inference happens on your device; nothing is sent externally.
- **No Tracking:** Your activities are never tracked.
